#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MissAV è§†é¢‘è¯¦æƒ…æŠ“å–è„šæœ¬

ç”¨æ³•ç¤ºä¾‹:
python video_detail_scraper.py --url "https://missav.live/cn/umso-612"
python video_detail_scraper.py --batch --limit 50  # æ‰¹é‡å¤„ç†æœªæŠ“å–çš„è§†é¢‘
python video_detail_scraper.py --batch  # æ‰¹é‡å¤„ç†æ‰€æœ‰æœªæŠ“å–çš„è§†é¢‘
python video_detail_scraper.py --url "..." --no-save  # ä»…æµ‹è¯•ä¸ä¿å­˜
python video_detail_scraper.py --update-subtitle-status  # æ›´æ–°æ‰€æœ‰è§†é¢‘çš„å­—å¹•å­˜åœ¨çŠ¶æ€


åŠŸèƒ½:
- è¾“å…¥: è§†é¢‘è¯¦æƒ…é¡µé¢URL æˆ– æ‰¹é‡å¤„ç†æ¨¡å¼
- å¤„ç†: æŠ“å–è§†é¢‘çš„è¯¦æƒ…æè¿°å’Œå…ƒæ•°æ®ä¿¡æ¯
- è¾“å‡º: æ§åˆ¶å°æ‰“å°è¯¦æƒ…ä¿¡æ¯ï¼Œå¹¶ä¿å­˜åˆ°æ•°æ®åº“

æ³¨æ„äº‹é¡¹:
- éœ€è¦å…ˆä½¿ç”¨gensession.txtä¸­çš„å‘½ä»¤ç”Ÿæˆsession_videoID.json
- ä½¿ç”¨Playwrightè¿›è¡Œç½‘é¡µæŠ“å–ï¼Œæ”¯æŒåçˆ¬æœºåˆ¶
- æ”¯æŒæ–­ç‚¹ç»­æŠ“ï¼Œé¿å…é‡å¤æŠ“å–å·²å¤„ç†çš„è§†é¢‘
"""

import argparse
import json
import os
import re
import time
import requests
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import signal
import sys

from playwright.sync_api import Playwright, sync_playwright, Page, BrowserContext
from playwright_stealth.stealth import stealth_sync
from bs4 import BeautifulSoup

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
from database_manager import DatabaseManager

# é…ç½®ç®¡ç†
BACKEND_CONFIG = {
    'local': 'http://localhost:8000',  # ä¿®æ­£ç«¯å£ä¸º8000
    'production': 'https://api.sub-dog.top'  # çº¿ä¸Šåç«¯APIåœ°å€
}

# å½“å‰ç¯å¢ƒé…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è®¾ç½®ï¼‰
CURRENT_ENV = os.getenv('BACKEND_ENV', 'production')
BACKEND_BASE_URL = BACKEND_CONFIG.get(CURRENT_ENV, BACKEND_CONFIG['production'])

# æ‰“å°å½“å‰ä½¿ç”¨çš„åç«¯é…ç½®
print(f"ğŸ”§ å½“å‰ç¯å¢ƒ: {CURRENT_ENV}")
print(f"ğŸŒ åç«¯APIåœ°å€: {BACKEND_BASE_URL}")


# å…¨å±€å˜é‡ç”¨äºæ§åˆ¶ç¨‹åºé€€å‡º
_shutdown_event = threading.Event()
_executor = None

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    print(f"\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· ({signum})ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    _shutdown_event.set()
    
    # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„çº¿ç¨‹æ± ï¼Œå°è¯•å…³é—­
    global _executor
    if _executor:
        print("â³ æ­£åœ¨å…³é—­çº¿ç¨‹æ± ...")
        try:
            # å–æ¶ˆæ‰€æœ‰æœªå¼€å§‹çš„ä»»åŠ¡
            _executor.shutdown(wait=False)
            print("âœ… çº¿ç¨‹æ± å·²å…³é—­")
        except Exception as e:
            print(f"âš ï¸ å…³é—­çº¿ç¨‹æ± æ—¶å‡ºç°å¼‚å¸¸: {e}")
    
    print("ğŸ‘‹ ç¨‹åºå³å°†é€€å‡º...")
    # ä¸è¦ç«‹å³è°ƒç”¨sys.exitï¼Œè®©ä¸»ç¨‹åºè‡ªç„¶ç»“æŸ

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
if hasattr(signal, 'SIGTERM'):
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·


# å…¨å±€ä¼šè¯å¯¹è±¡å’Œé”
_session_lock = threading.Lock()
_global_session = None

def get_global_session():
    """è·å–å…¨å±€ä¼šè¯å¯¹è±¡ï¼Œé¿å…é‡å¤åˆ›å»º"""
    global _global_session
    with _session_lock:
        if _global_session is None:
            _global_session = requests.Session()
            
            # ç¦ç”¨urllib3çš„SSLè­¦å‘Š
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            # å¦‚æœæ˜¯ç”Ÿäº§ç¯å¢ƒï¼Œé…ç½®SOCKS5ä»£ç†
            if CURRENT_ENV == 'production':
                proxies = {
                    'http': 'socks5://127.0.0.1:7890',
                    'https': 'socks5://127.0.0.1:7890'
                }
                _global_session.proxies.update(proxies)
                _global_session.trust_env = False
                print(f"ğŸ”— ä½¿ç”¨SOCKS5ä»£ç†: 127.0.0.1:7890")
            else:
                _global_session.trust_env = False
                _global_session.proxies = {}
                
            # è®¾ç½®é€šç”¨è¯·æ±‚å¤´
            _global_session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
        return _global_session


def check_subtitle_exists(video_id: str) -> bool:
    """
    é€šè¿‡HTTP APIæ£€æŸ¥å­—å¹•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    
    Args:
        video_id: è§†é¢‘ID
        
    Returns:
        bool: å­—å¹•æ˜¯å¦å­˜åœ¨
    """
    try:
        # æ„å»ºAPI URL
        api_url = f"{BACKEND_BASE_URL}/api/subtitles/exists/{video_id}"
        
        # ä½¿ç”¨å…¨å±€ä¼šè¯
        session = get_global_session()
        
        # å‘é€GETè¯·æ±‚
        response = session.get(
            api_url, 
            timeout=30,
            verify=False
        )
        
        if response.status_code == 200:
            data = response.json()
            return data.get('exists', False)
        else:
            print(f"æ£€æŸ¥å­—å¹•å­˜åœ¨æ€§å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"æ£€æŸ¥å­—å¹•å­˜åœ¨æ€§æ—¶ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"æ£€æŸ¥å­—å¹•å­˜åœ¨æ€§æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False


def load_session_if_exists(session_file: str = "./session_videoID.json") -> Optional[Dict]:
    """åŠ è½½å·²ä¿å­˜çš„sessionçŠ¶æ€"""
    if os.path.exists(session_file):
        try:
            with open(session_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"åŠ è½½sessionæ–‡ä»¶å¤±è´¥: {e}")
    return None


def setup_playwright_page(playwright: Playwright, session_file: str = "./session_videoID.json") -> tuple[Page, BrowserContext]:
    """è®¾ç½®Playwrighté¡µé¢å’Œæµè§ˆå™¨ä¸Šä¸‹æ–‡"""
    # å¯åŠ¨æµè§ˆå™¨
    try:
        browser = playwright.chromium.launch(
            headless=False,
            channel="msedge",
            args=[
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",
                "--disable-web-security",
                "--disable-features=VizDisplayCompositor",
                "--disable-dev-shm-usage",
                "--no-first-run",
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            ]
        )
    except Exception as e:
        print(f"Edgeæµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼Œå›é€€åˆ°Chromium: {e}")
        browser = playwright.chromium.launch(
            headless=False,
            args=[
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",
                "--disable-web-security",
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
            ]
        )
    
    # åŠ è½½sessionçŠ¶æ€
    session_state = load_session_if_exists(session_file)
    if not session_state:
        print(f"æœªå‘ç° {session_file}ï¼Œè¯·å…ˆä½¿ç”¨ Playwright codegen ç™»å½•å¹¶ä¿å­˜ä¼šè¯")
        print(f"ç¤ºä¾‹ï¼špython -m playwright codegen --channel=msedge --save-storage={session_file} https://missav.live/")
        browser.close()
        raise RuntimeError("Sessionæ–‡ä»¶ä¸å­˜åœ¨")
    
    # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
    try:
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers={
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'
            },
            accept_downloads=True
        )
        
        # æ‰‹åŠ¨æ·»åŠ cookies
        if 'cookies' in session_state:
            context.add_cookies(session_state['cookies'])
            
    except Exception as e:
        print(f"åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        # å¦‚æœæ‰‹åŠ¨æ·»åŠ cookieså¤±è´¥ï¼Œå°è¯•ä½¿ç”¨storage_state
        try:
            context = browser.new_context(
                storage_state=session_state,
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080},
                extra_http_headers={
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8'
                },
                accept_downloads=True
            )
        except Exception as e2:
            print(f"ä½¿ç”¨storage_stateåˆ›å»ºä¸Šä¸‹æ–‡ä¹Ÿå¤±è´¥: {e2}")
            browser.close()
            raise RuntimeError(f"æ— æ³•åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡: {e2}")
    
    page = context.new_page()
    stealth_sync(page)
    
    # æ·»åŠ åæ£€æµ‹è„šæœ¬
    page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {
            get: () => undefined,
        });
        
        Object.defineProperty(navigator, 'plugins', {
            get: () => [1, 2, 3, 4, 5],
        });
        
        Object.defineProperty(navigator, 'languages', {
            get: () => ['zh-CN', 'zh', 'en'],
        });
        
        window.chrome = {
            runtime: {},
        };
    """)
    
    return page, context


def extract_video_description(soup: BeautifulSoup) -> str:
    """æå–è§†é¢‘è¯¦æƒ…æè¿°"""
    # æŸ¥æ‰¾åŒ…å« line-clamp-2 ç±»çš„ div å…ƒç´ 
    desc_div = soup.find("div", class_=lambda x: x and "line-clamp-2" in x)
    if desc_div:
        return desc_div.get_text(strip=True)
    return ""


def extract_cover_url(soup: BeautifulSoup) -> str:
    """æå–è§†é¢‘å°é¢URL - ä¼˜åŒ–ç‰ˆæœ¬"""
    # ä¼˜åŒ–ï¼šç›´æ¥æŸ¥æ‰¾æœ€å¸¸è§çš„æƒ…å†µ - videoæ ‡ç­¾çš„data-posterå±æ€§
    video_tag = soup.find("video", {"data-poster": True})
    if video_tag:
        poster_url = video_tag.get("data-poster")
        if poster_url and poster_url.strip():
            return poster_url.strip()
    
    # å¤‡ç”¨æ–¹æ¡ˆ1: æŸ¥æ‰¾videoæ ‡ç­¾çš„posterå±æ€§
    video_tag = soup.find("video", poster=True)
    if video_tag:
        poster_url = video_tag.get("poster")
        if poster_url and poster_url.strip():
            return poster_url.strip()
    
    # å¤‡ç”¨æ–¹æ¡ˆ2: æŸ¥æ‰¾åŒ…å« plyr__poster ç±»çš„ div å…ƒç´ 
    poster_div = soup.find("div", class_="plyr__poster")
    if poster_div:
        style = poster_div.get("style", "")
        if "background-image" in style:
            import re
            # ä» style å±æ€§ä¸­æå– URL
            match = re.search(r'url\(&quot;([^&]+)&quot;\)', style)
            if match:
                return match.group(1)
            match = re.search(r'url\(["\']([^"\']+)["\']\)', style)
            if match:
                return match.group(1)
    
    return ""


def extract_video_metadata(soup: BeautifulSoup) -> Dict[str, any]:
    """æå–è§†é¢‘å…ƒæ•°æ®ä¿¡æ¯"""
    metadata = {
        "release_date": "",
        "video_id": "",
        "title": "",
        "actresses": [],
        "actors": [],
        "genres": [],
        "series": "",
        "maker": "",
        "director": "",
        "label": ""
    }
    
    # æŸ¥æ‰¾åŒ…å«å…ƒæ•°æ®çš„ div.space-y-2
    metadata_div = soup.find("div", class_="space-y-2")
    if not metadata_div:
        return metadata
    
    # æå–å„ä¸ªå­—æ®µ
    for div in metadata_div.find_all("div", class_="text-secondary"):
        text = div.get_text(strip=True)
        
        if text.startswith("å‘è¡Œæ—¥æœŸ:"):
            time_elem = div.find("time")
            if time_elem:
                metadata["release_date"] = time_elem.get("datetime", "").split("T")[0]
        
        elif text.startswith("ç•ªå·:"):
            span = div.find("span", class_="font-medium")
            if span:
                metadata["video_id"] = span.get_text(strip=True)
        
        elif text.startswith("æ ‡é¢˜:"):
            span = div.find("span", class_="font-medium")
            if span:
                metadata["title"] = span.get_text(strip=True)
        
        elif text.startswith("å¥³ä¼˜:"):
            actresses = []
            for a in div.find_all("a", class_="text-nord13"):
                actresses.append(a.get_text(strip=True))
            metadata["actresses"] = actresses
        
        elif text.startswith("ç”·ä¼˜:"):
            actors = []
            for a in div.find_all("a", class_="text-nord13"):
                actors.append(a.get_text(strip=True))
            metadata["actors"] = actors
        
        elif text.startswith("ç±»å‹:"):
            genres = []
            for a in div.find_all("a", class_="text-nord13"):
                genres.append(a.get_text(strip=True))
            metadata["genres"] = genres
        
        elif text.startswith("ç³»åˆ—:"):
            a = div.find("a", class_="text-nord13")
            if a:
                metadata["series"] = a.get_text(strip=True)
        
        elif text.startswith("å‘è¡Œå•†:"):
            a = div.find("a", class_="text-nord13")
            if a:
                metadata["maker"] = a.get_text(strip=True)
        
        elif text.startswith("å¯¼æ¼”:"):
            a = div.find("a", class_="text-nord13")
            if a:
                metadata["director"] = a.get_text(strip=True)
        
        elif text.startswith("æ ‡ç±¤:"):
            a = div.find("a", class_="text-nord13")
            if a:
                metadata["label"] = a.get_text(strip=True)
    
    return metadata


def extract_video_id_from_url(url: str) -> Optional[str]:
    """ä»URLä¸­æå–è§†é¢‘ID"""
    try:
        # è§£æURLè·¯å¾„
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # æå–æœ€åä¸€ä¸ªè·¯å¾„æ®µä½œä¸ºè§†é¢‘ID
        # ä¾‹å¦‚: https://missav.live/cn/sone-891 -> sone-891
        if '/' in path:
            video_id = path.split('/')[-1]
        else:
            video_id = path
            
        # å¤„ç†åŒ…å«åç¼€çš„è§†é¢‘IDï¼ˆå¦‚ uncensored-leak, chinese-subtitle ç­‰ï¼‰
        # ç§»é™¤å¸¸è§çš„åç¼€ï¼Œä¿ç•™åŸºç¡€è§†é¢‘ID
        suffixes_to_remove = [
            '-uncensored-leak',
            '-chinese-subtitle', 
            '-leak',
            '-uncensored'
        ]
        
        for suffix in suffixes_to_remove:
            if video_id.lower().endswith(suffix):
                video_id = video_id[:-len(suffix)]
                break
        
        # éªŒè¯è§†é¢‘IDæ ¼å¼ï¼ˆåŸºæœ¬éªŒè¯ï¼‰
        if video_id and len(video_id) > 2:
            return video_id.upper()  # ç»Ÿä¸€è½¬ä¸ºå¤§å†™
        
        return None
    except Exception as e:
        print(f"æå–è§†é¢‘IDå¤±è´¥: {e}")
        return None


def save_video_details_to_db(video_id: str, metadata: Dict, cover_url: str, description: str, video_url: str = None):
    """å°†è§†é¢‘è¯¦æƒ…ä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        db_manager = DatabaseManager("./database/actresses.db")
        
        # å¦‚æœæä¾›äº†video_urlï¼Œä¼˜å…ˆæŒ‰URLæŸ¥æ‰¾è®°å½•ï¼›å¦åˆ™æŒ‰video_idæŸ¥æ‰¾
        if video_url:
            records = db_manager.find_videos_by_url(video_url)
        else:
            records = db_manager.find_videos_by_id(video_id)
        
        if not records:
            if video_url:
                print(f"è­¦å‘Š: æ•°æ®åº“ä¸­æœªæ‰¾åˆ°URLä¸º {video_url} çš„è®°å½•")
            else:
                print(f"è­¦å‘Š: æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è§†é¢‘IDä¸º {video_id} çš„è®°å½•")
            return False
        
        # æ£€æŸ¥å­—å¹•æ˜¯å¦å­˜åœ¨
        subtitle_exists = check_subtitle_exists(video_id)
        
        # å‡†å¤‡è¯¦æƒ…æ•°æ®
        details = {
            'release_date': metadata.get('release_date', ''),
            'cover_url': cover_url,
            'description': description,
            'actresses': metadata.get('actresses', []),
            'actors': metadata.get('actors', []),
            'genres': metadata.get('genres', []),
            'series': metadata.get('series', ''),
            'maker': metadata.get('maker', ''),
            'director': metadata.get('director', ''),
            'subtitle_downloaded': 1 if subtitle_exists else 0
        }
        
        # æ›´æ–°æ‰€æœ‰åŒ¹é…çš„è®°å½•
        updated_count = 0
        for record in records:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æŠ“å–è¿‡
            if record.get('detail_scraped'):
                print(f"è®°å½• {record['id']} å·²ç»æŠ“å–è¿‡è¯¦æƒ…ï¼Œè·³è¿‡")
                continue
                
            db_manager.update_video_details(record['id'], details)
            updated_count += 1
            subtitle_status = "æœ‰å­—å¹•" if subtitle_exists else "æ— å­—å¹•"
            print(f"å·²æ›´æ–°è®°å½• {record['id']}: {record['actress_name']} - {record['video_title']} ({subtitle_status})")
        
        print(f"æˆåŠŸæ›´æ–° {updated_count} æ¡è®°å½•")
        return updated_count > 0
        
    except Exception as e:
        print(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return False


def scrape_single_video(url: str, timeout: int = 30, save_to_db: bool = True) -> bool:
    """æŠ“å–å•ä¸ªè§†é¢‘çš„è¯¦æƒ…"""
    print(f"å¼€å§‹æŠ“å–è§†é¢‘è¯¦æƒ…: {url}")
    
    # æå–è§†é¢‘ID
    video_id = extract_video_id_from_url(url)
    if not video_id:
        print("æ— æ³•ä»URLä¸­æå–è§†é¢‘ID")
        return False
    
    print(f"æå–åˆ°è§†é¢‘ID: {video_id}")
    
    with sync_playwright() as playwright:
        page, context = setup_playwright_page(playwright)
        
        try:
            # è®¿é—®é¡µé¢å¹¶è·å–å†…å®¹
            print(f"æ­£åœ¨è®¿é—®: {url}")
            response = page.goto(url, wait_until="domcontentloaded", timeout=timeout * 1000)
            
            if response and response.status != 200:
                print(f"é¡µé¢è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                return False
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨domcontentloadedæ›¿ä»£networkidleï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
            page.wait_for_load_state('domcontentloaded', timeout=5000)
            
            # ä¼˜åŒ–ï¼šç›´æ¥å°è¯•æŸ¥æ‰¾videoå…ƒç´ ï¼Œå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…
            try:
                page.wait_for_selector('video', timeout=2000)
            except:
                pass  # å¦‚æœæ²¡æœ‰videoå…ƒç´ ä¹Ÿç»§ç»­æ‰§è¡Œ
            
            # è·å–é¡µé¢å†…å®¹
            content = page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # æŠ“å–è§†é¢‘è¯¦æƒ…
            description = extract_video_description(soup)
            metadata = extract_video_metadata(soup)
            cover_url = extract_cover_url(soup)
            
            # æ‰“å°è¯¦æƒ…ä¿¡æ¯
            print_video_details(video_id, description, metadata, cover_url)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_to_db:
                success = save_video_details_to_db(video_id, metadata, cover_url, description, url)
                return success
            
            return True
            
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}")
            return False
        
        finally:
            context.close()


def scrape_batch_videos(limit: int = 100) -> Dict[str, int]:
    """æ‰¹é‡æŠ“å–æœªå¤„ç†çš„è§†é¢‘è¯¦æƒ…"""
    try:
        db_manager = DatabaseManager("./database/actresses.db")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = db_manager.get_video_details_stats()
        print(f"æ•°æ®åº“ç»Ÿè®¡: æ€»è®¡ {stats['total']} æ¡è®°å½•ï¼Œå·²æŠ“å– {stats['scraped']} æ¡ï¼ŒæœªæŠ“å– {stats['unscraped']} æ¡")
        
        if stats['unscraped'] == 0:
            print("æ‰€æœ‰è§†é¢‘è¯¦æƒ…å·²æŠ“å–å®Œæˆ")
            return {'total': 0, 'success': 0, 'failed': 0}
        
        # è·å–æœªæŠ“å–çš„è§†é¢‘è®°å½•
        unscraped_videos = db_manager.get_unscraped_videos(limit)
        print(f"æœ¬æ¬¡å°†å¤„ç† {len(unscraped_videos)} æ¡è®°å½•")
        
        success_count = 0
        failed_count = 0
        
        with sync_playwright() as playwright:
            page, context = setup_playwright_page(playwright)
            
            try:
                for i, video in enumerate(unscraped_videos, 1):
                    print(f"\n[{i}/{len(unscraped_videos)}] å¤„ç†è§†é¢‘: {video['video_id']}")
                    
                    try:
                        # æ„å»ºè§†é¢‘URLï¼ˆå‡è®¾ä½¿ç”¨missav.liveåŸŸåï¼‰
                        video_url = f"https://missav.live/cn/{video['video_id'].lower()}"
                        
                        # è®¿é—®é¡µé¢å¹¶è·å–å†…å®¹
                        response = page.goto(video_url, wait_until="domcontentloaded", timeout=30000)
                        
                        if response and response.status != 200:
                            print(f"é¡µé¢è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                            failed_count += 1
                            continue
                        
                        # ä¼˜åŒ–ï¼šä½¿ç”¨domcontentloadedæ›¿ä»£networkidleï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
                        page.wait_for_load_state('domcontentloaded', timeout=5000)
                        
                        # ä¼˜åŒ–ï¼šç›´æ¥å°è¯•æŸ¥æ‰¾videoå…ƒç´ ï¼Œå‡å°‘ä¸å¿…è¦çš„ç­‰å¾…
                        try:
                            page.wait_for_selector('video', timeout=2000)
                        except:
                            pass  # å¦‚æœæ²¡æœ‰videoå…ƒç´ ä¹Ÿç»§ç»­æ‰§è¡Œ
                        
                        # è·å–é¡µé¢å†…å®¹
                        content = page.content()
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # æŠ“å–è¯¦æƒ…
                        description = extract_video_description(soup)
                        metadata = extract_video_metadata(soup)
                        cover_url = extract_cover_url(soup)
                        
                        # æ£€æŸ¥å­—å¹•æ˜¯å¦å­˜åœ¨
                        subtitle_exists = check_subtitle_exists(video['video_id'])
                        
                        # å‡†å¤‡è¯¦æƒ…æ•°æ®
                        details = {
                            'release_date': metadata.get('release_date', ''),
                            'cover_url': cover_url,
                            'description': description,
                            'actresses': metadata.get('actresses', []),
                            'actors': metadata.get('actors', []),
                            'genres': metadata.get('genres', []),
                            'series': metadata.get('series', ''),
                            'maker': metadata.get('maker', ''),
                            'director': metadata.get('director', ''),
                            'subtitle_downloaded': 1 if subtitle_exists else 0
                        }
                        
                        # æ›´æ–°æ•°æ®åº“
                        db_manager.update_video_details(video['id'], details)
                        success_count += 1
                        subtitle_status = "æœ‰å­—å¹•" if subtitle_exists else "æ— å­—å¹•"
                        print(f"âœ“ æˆåŠŸå¤„ç†: {video['actress_name']} - {video['video_title']} ({subtitle_status})")
                        
                        # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                        time.sleep(2)
                        
                    except Exception as e:
                        failed_count += 1
                        print(f"âœ— å¤„ç†å¤±è´¥: {video['video_id']} - {e}")
                        continue
                        
            finally:
                context.close()
        
        result = {
            'total': len(unscraped_videos),
            'success': success_count,
            'failed': failed_count
        }
        
        print(f"\næ‰¹é‡å¤„ç†å®Œæˆ: æ€»è®¡ {result['total']} æ¡ï¼ŒæˆåŠŸ {result['success']} æ¡ï¼Œå¤±è´¥ {result['failed']} æ¡")
        return result
        
    except Exception as e:
        print(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        return {'total': 0, 'success': 0, 'failed': 0}


def print_video_details(video_id: str, description: str, metadata: Dict, cover_url: str):
    """æ‰“å°è§†é¢‘è¯¦æƒ…ä¿¡æ¯"""
    print("\n" + "="*60)
    print("è§†é¢‘è¯¦æƒ…æŠ“å–ç»“æœ")
    print("="*60)
    
    print(f"\nã€è§†é¢‘IDã€‘")
    print(video_id)
    
    print(f"\nã€æ ‡é¢˜ã€‘")
    print(metadata["title"])
    
    print(f"\nã€å‘å¸ƒæ—¥æœŸã€‘")
    print(metadata["release_date"])
    
    print(f"\nã€æ—¶é•¿ã€‘")
    print(metadata.get("duration", "æœªçŸ¥"))
    
    print(f"\nã€å°é¢å›¾ç‰‡ã€‘")
    print(cover_url)
    
    print(f"\nã€æè¿°ã€‘")
    print(description)
    
    print(f"\nã€å¥³ä¼˜ã€‘")
    for actress in metadata["actresses"]:
        print(f"  - {actress}")
    
    print(f"\nã€ç”·ä¼˜ã€‘")
    for actor in metadata["actors"]:
        print(f"  - {actor}")
    
    print(f"\nã€ç±»å‹ã€‘")
    for genre in metadata["genres"]:
        print(f"  - {genre}")
    
    print(f"\nã€ç³»åˆ—ã€‘")
    print(metadata["series"])
    
    print(f"\nã€å‘è¡Œå•†ã€‘")
    print(metadata["maker"])
    
    print(f"\nã€å¯¼æ¼”ã€‘")
    print(metadata["director"])
    
    print("\n" + "="*60)


def process_single_video(video_data, index, total_videos):
    """
    å¤„ç†å•ä¸ªè§†é¢‘çš„å­—å¹•çŠ¶æ€æ£€æŸ¥
    
    Args:
        video_data: åŒ…å«è§†é¢‘ä¿¡æ¯çš„å…ƒç»„ (video, db_manager)
        index: å½“å‰è§†é¢‘ç´¢å¼•
        total_videos: æ€»è§†é¢‘æ•°é‡
        
    Returns:
        dict: å¤„ç†ç»“æœ
    """
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
    if _shutdown_event.is_set():
        return {'index': index, 'video_id': 'SHUTDOWN', 'success': False, 'error': 'Program shutdown'}
    
    video, db_manager = video_data
    video_id = video.get('video_id')
    video_title = video.get('video_title', 'Unknown')
    
    result = {
        'index': index,
        'video_id': video_id,
        'success': False,
        'subtitle_exists': False,
        'error': None
    }
    
    if not video_id:
        result['error'] = f"è§†é¢‘IDä¸ºç©º - {video_title}"
        return result
    
    try:
        # æ£€æŸ¥å­—å¹•æ˜¯å¦å­˜åœ¨
        subtitle_exists = check_subtitle_exists(video_id)
        
        # å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
        if _shutdown_event.is_set():
            return {'index': index, 'video_id': 'SHUTDOWN', 'success': False, 'error': 'Program shutdown'}
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„å­—å¹•çŠ¶æ€ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
        max_retries = 3
        success = False
        last_error = None
        
        for attempt in range(max_retries):
            try:
                success = db_manager.update_subtitle_status(video_id, subtitle_exists)
                if success:
                    break
                else:
                    last_error = f"æ•°æ®åº“æ›´æ–°å¤±è´¥ - æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•æˆ–æ›´æ–°å¤±è´¥"
                    if attempt < max_retries - 1:
                        import time
                        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
            except Exception as db_error:
                last_error = f"æ•°æ®åº“æ“ä½œå¼‚å¸¸: {str(db_error)}"
                if attempt < max_retries - 1:
                    import time
                    time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•è¯¦ç»†é”™è¯¯
                    import traceback
                    last_error = f"æ•°æ®åº“æ“ä½œå¼‚å¸¸ (å°è¯•{max_retries}æ¬¡åå¤±è´¥): {str(db_error)}\n{traceback.format_exc()}"
        
        if success:
            result['success'] = True
            result['subtitle_exists'] = subtitle_exists
        else:
            result['error'] = last_error or "æ•°æ®åº“æ›´æ–°å¤±è´¥"
            
    except Exception as e:
        import traceback
        result['error'] = f"å¤„ç†å¼‚å¸¸: {str(e)}\n{traceback.format_exc()}"
    
    return result


def update_all_subtitle_status():
    """
    æ›´æ–°æ‰€æœ‰è§†é¢‘çš„å­—å¹•å­˜åœ¨çŠ¶æ€ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰
    éå†æ•°æ®åº“ä¸­çš„æ‰€æœ‰è§†é¢‘è®°å½•ï¼Œå¹¶å‘æ£€æŸ¥å­—å¹•æ˜¯å¦å­˜åœ¨å¹¶æ›´æ–°çŠ¶æ€
    """
    global _executor
    
    print("å¼€å§‹æ›´æ–°æ‰€æœ‰è§†é¢‘çš„å­—å¹•å­˜åœ¨çŠ¶æ€...")
    
    # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
    db_manager = DatabaseManager()
    
    # è·å–æ‰€æœ‰è§†é¢‘è®°å½•
    all_videos = db_manager.get_all_videos()
    
    if not all_videos:
        print("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘è®°å½•")
        return
    
    total_videos = len(all_videos)
    print(f"æ‰¾åˆ° {total_videos} ä¸ªè§†é¢‘è®°å½•ï¼Œå¼€å§‹æ£€æŸ¥å­—å¹•çŠ¶æ€...")
    
    # ç»Ÿè®¡å˜é‡
    updated_count = 0
    error_count = 0
    subtitle_exists_count = 0
    subtitle_not_exists_count = 0
    
    # å‡†å¤‡æ•°æ®ï¼šæ¯ä¸ªä»»åŠ¡åŒ…å«è§†é¢‘ä¿¡æ¯å’Œæ•°æ®åº“ç®¡ç†å™¨
    video_data_list = [(video, db_manager) for video in all_videos]
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    max_workers = min(10, total_videos)  # æœ€å¤š10ä¸ªçº¿ç¨‹
    print(f"ğŸš€ ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶å‘å¤„ç†...")
    print("ğŸ’¡ æŒ‰ Ctrl+C å¯éšæ—¶ä¸­æ–­ç¨‹åº")
    
    try:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            _executor = executor  # ä¿å­˜æ‰§è¡Œå™¨å¼•ç”¨
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_video, video_data_list[i], i+1, total_videos): i+1 
                for i in range(total_videos)
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_index):
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                if _shutdown_event.is_set():
                    print("ğŸ›‘ æ£€æµ‹åˆ°é€€å‡ºä¿¡å·ï¼Œåœæ­¢å¤„ç†...")
                    break
                    
                index = future_to_index[future]
                
                try:
                    result = future.result(timeout=1)  # æ·»åŠ è¶…æ—¶é¿å…é˜»å¡
                    
                    # è·³è¿‡å› ç¨‹åºå…³é—­äº§ç”Ÿçš„ç»“æœ
                    if result.get('video_id') == 'SHUTDOWN':
                        continue
                    
                    if result['success']:
                        status_text = "å­˜åœ¨" if result['subtitle_exists'] else "ä¸å­˜åœ¨"
                        print(f"[{result['index']}/{total_videos}] æ›´æ–°æˆåŠŸï¼š{result['video_id']} - å­—å¹•{status_text}")
                        updated_count += 1
                        
                        # ç»Ÿè®¡å­—å¹•å­˜åœ¨æƒ…å†µ
                        if result['subtitle_exists']:
                            subtitle_exists_count += 1
                        else:
                            subtitle_not_exists_count += 1
                    else:
                        print(f"[{result['index']}/{total_videos}] æ›´æ–°å¤±è´¥ï¼š{result['video_id']} - {result['error']}")
                        error_count += 1
                        
                except Exception as e:
                    print(f"[{index}/{total_videos}] å¤„ç†å¼‚å¸¸ï¼š{e}")
                    error_count += 1
                
                # æ¯å¤„ç†100ä¸ªè§†é¢‘æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                processed = updated_count + error_count
                if processed % 100 == 0:
                    print(f"ğŸ“Š è¿›åº¦ï¼šå·²å¤„ç† {processed}/{total_videos} ä¸ªè§†é¢‘ï¼ŒæˆåŠŸæ›´æ–° {updated_count} ä¸ªï¼Œå¤±è´¥ {error_count} ä¸ª")
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        _shutdown_event.set()  # ç¡®ä¿è®¾ç½®é€€å‡ºæ ‡å¿—
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™ï¼š{e}")
    finally:
        # ç¡®ä¿çº¿ç¨‹æ± è¢«æ­£ç¡®å…³é—­
        if _executor:
            try:
                print("ğŸ”„ æ­£åœ¨ç­‰å¾…çº¿ç¨‹æ± å®Œå…¨å…³é—­...")
                _executor.shutdown(wait=True)  # ThreadPoolExecutor.shutdown()ä¸æ”¯æŒtimeoutå‚æ•°
                print("âœ… çº¿ç¨‹æ± å·²å®Œå…¨å…³é—­")
            except Exception as e:
                print(f"âš ï¸ å…³é—­çº¿ç¨‹æ± æ—¶å‡ºç°å¼‚å¸¸: {e}")
            finally:
                _executor = None  # æ¸…é™¤æ‰§è¡Œå™¨å¼•ç”¨
    
    if not _shutdown_event.is_set():
        print(f"\nâœ… å­—å¹•çŠ¶æ€æ›´æ–°å®Œæˆï¼")
        print(f"æ€»è®¡ï¼š{total_videos} ä¸ªè§†é¢‘")
        print(f"æˆåŠŸæ›´æ–°ï¼š{updated_count} ä¸ª")
        print(f"å¤±è´¥ï¼š{error_count} ä¸ª")
        print(f"å­—å¹•å­˜åœ¨ï¼š{subtitle_exists_count} ä¸ª")
        print(f"å­—å¹•ä¸å­˜åœ¨ï¼š{subtitle_not_exists_count} ä¸ª")
    else:
        print(f"\nâš ï¸ ç¨‹åºè¢«ä¸­æ–­ï¼Œéƒ¨åˆ†å¤„ç†å®Œæˆ")
        print(f"å·²å¤„ç†ï¼š{updated_count + error_count} ä¸ªè§†é¢‘")
        print(f"æˆåŠŸæ›´æ–°ï¼š{updated_count} ä¸ª")
        print(f"å¤±è´¥ï¼š{error_count} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    parser = argparse.ArgumentParser(description="MissAV è§†é¢‘è¯¦æƒ…æŠ“å–è„šæœ¬")
    parser.add_argument("--url", help="è§†é¢‘è¯¦æƒ…é¡µé¢URL")
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡å¤„ç†æ¨¡å¼")
    parser.add_argument("--limit", type=int, default=100, help="æ‰¹é‡å¤„ç†æ—¶çš„è®°å½•æ•°é™åˆ¶")
    parser.add_argument("--timeout", type=int, default=30, help="é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--no-save", action="store_true", help="ä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä»…æ‰“å°ç»“æœ")
    parser.add_argument("--update-subtitle-status", action="store_true", help="æ›´æ–°æ‰€æœ‰è§†é¢‘çš„å­—å¹•å­˜åœ¨çŠ¶æ€")
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.batch and not args.url and not args.update_subtitle_status:
        parser.error("å¿…é¡»æŒ‡å®š --urlã€ä½¿ç”¨ --batch æ¨¡å¼æˆ–ä½¿ç”¨ --update-subtitle-status")
    
    if sum([bool(args.batch), bool(args.url), bool(args.update_subtitle_status)]) > 1:
        parser.error("--batchã€--url å’Œ --update-subtitle-status ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    
    try:
        if args.update_subtitle_status:
            # å­—å¹•çŠ¶æ€æ›´æ–°æ¨¡å¼
            print("å¯åŠ¨å­—å¹•çŠ¶æ€æ›´æ–°æ¨¡å¼...")
            update_all_subtitle_status()
        elif args.batch:
            # æ‰¹é‡å¤„ç†æ¨¡å¼
            print("å¯åŠ¨æ‰¹é‡å¤„ç†æ¨¡å¼...")
            scrape_batch_videos(args.limit)
        else:
            # å•ä¸ªURLå¤„ç†æ¨¡å¼
            scrape_single_video(args.url, args.timeout, not args.no_save)
            
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    main()